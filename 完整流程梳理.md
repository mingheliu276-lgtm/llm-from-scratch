# 完整流程梳理：从数据到训练到使用

本文档梳理整个项目的完整流程：数据抓取 → 数据处理 → 训练 → 推理/对话。

---

## 一、数据抓取与处理

### 1.1 真实数据（WikiText-2）

**脚本**：`scripts/train_with_real_data.py` 中的 `load_wikitext2()`

**流程**：

```python
# 步骤1：从 HuggingFace 下载数据集
from datasets import load_dataset
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
# 返回：train / validation / test 三个 split

# 步骤2：加载 GPT-2 tokenizer（词表大小 50257）
from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# 步骤3：对文本进行 tokenization
def tokenize_function(examples):
    tokenized = tokenizer(
        examples["text"],
        truncation=True,        # 超过 max_length 截断
        max_length=max_seq_len + 1,  # +1 因为后面要生成 labels
        padding="max_length",   # 不足的用 pad_token 补齐
        return_tensors="pt"
    )
    return {"input_ids": tokenized["input_ids"].squeeze(0)}

# 步骤4：批量处理所有样本
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,  # 批量处理，加速
    remove_columns=dataset["train"].column_names
)

# 步骤5：转换为 PyTorch Dataset
class TokenizedDataset(Dataset):
    def __getitem__(self, idx):
        item = self.data[idx]
        input_ids = item["input_ids"]
        if not isinstance(input_ids, torch.Tensor):
            input_ids = torch.tensor(input_ids, dtype=torch.long)
        return input_ids  # 返回单个 tensor，避免 DataLoader collate 出错
```

**输出**：
- `train_ds`：训练集（约 23767 个样本）
- `val_ds`：验证集（约 2461 个样本）
- `vocab_size`：50257（GPT-2 词表大小）

### 1.2 虚拟数据（用于快速测试）

**脚本**：`scripts/train.py` 中的 `create_dummy_dataset()`

```python
# 生成随机 token 序列
data = torch.randint(1, vocab_size, (num_samples, seq_len))
dataset = TensorDataset(data)
```

**用途**：仅用于验证训练流程是否正常，不追求「学会语言」。

---

## 二、训练流程

### 2.1 创建模型

```python
from src.models.transformer import SimpleGPT

model = SimpleGPT(
    vocab_size=vocab_size,      # 50257（真实数据）或 10000（虚拟数据）
    d_model=256,                 # 模型维度
    num_heads=4,                 # 注意力头数
    num_layers=2,                # Transformer 层数
    max_seq_len=128,             # 最大序列长度
)
```

**模型结构**：
- `TransformerEncoder`：嵌入 + 位置编码 + N 层 Encoder
- `lm_head`：线性层，输出 vocab_size 个 logits

### 2.2 创建 DataLoader

```python
from torch.utils.data import DataLoader

train_loader = DataLoader(
    train_dataset,
    batch_size=4,
    shuffle=True,
    num_workers=0,  # Windows 上设为 0
)
val_loader = DataLoader(
    val_dataset,
    batch_size=4,
    shuffle=False,
    num_workers=0,
)
```

### 2.3 创建 Trainer

```python
from src.training.trainer import Trainer

trainer = Trainer(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=torch.optim.AdamW(model.parameters(), lr=1e-4),
    device="cuda",
    use_amp=True,  # 混合精度训练
    config={  # 保存配置，推理时自动匹配
        "vocab_size": vocab_size,
        "d_model": 256,
        "num_heads": 4,
        "num_layers": 2,
        "max_seq_len": 128,
    },
)
```

### 2.4 训练循环（Trainer.train_epoch）

**每个 batch 的流程**：

1. **数据准备**：
   ```python
   # batch 是 [B, seq_len] 的 token id
   input_ids = batch[:, :-1]  # [B, seq_len-1]：去掉最后一个 token
   labels = batch[:, 1:]       # [B, seq_len-1]：去掉第一个 token（预测下一个）
   ```

2. **前向传播**：
   ```python
   with autocast():  # 混合精度
       logits = model(input_ids)  # [B, seq_len-1, vocab_size]
       loss = criterion(logits.view(-1, vocab_size), labels.view(-1))
   ```

3. **反向传播**：
   ```python
   scaler.scale(loss).backward()  # 计算梯度
   ```

4. **更新参数**（每 `gradient_accumulation_steps` 步）：
   ```python
   scaler.unscale_(optimizer)
   clip_grad_norm_(model.parameters(), max_grad_norm=1.0)  # 梯度裁剪
   scaler.step(optimizer)  # 更新参数
   scaler.update()  # 更新 scaler
   optimizer.zero_grad()  # 清空梯度
   ```

5. **验证**（每个 epoch 结束后）：
   ```python
   model.eval()
   with torch.no_grad():
       for batch in val_loader:
           logits = model(input_ids)
           loss = criterion(...)
   ```

6. **保存检查点**：
   ```python
   checkpoint = {
       'model_state_dict': model.state_dict(),
       'optimizer_state_dict': optimizer.state_dict(),
       'epoch': epoch,
       'best_val_loss': best_val_loss,
       'config': config,  # 模型配置
   }
   torch.save(checkpoint, "checkpoints/best_model.pt")
   ```

### 2.5 运行训练

```bash
# 真实数据训练
python scripts/train_with_real_data.py \
    --dataset wikitext2 \
    --d_model 256 \
    --num_layers 2 \
    --num_heads 4 \
    --batch_size 4 \
    --num_epochs 5 \
    --use_amp

# 虚拟数据训练（快速测试）
python scripts/train.py \
    --d_model 256 \
    --num_layers 2 \
    --num_heads 4 \
    --batch_size 4 \
    --use_amp
```

**输出**：
- `checkpoints/best_model.pt`：最佳模型（验证损失最低）
- `checkpoints/checkpoint_epoch_X_step_Y.pt`：每个 epoch 的检查点

---

## 三、推理/使用流程

### 3.1 加载模型（inference.py / chat.py）

```python
# 步骤1：加载 checkpoint
checkpoint = torch.load("checkpoints/best_model.pt", map_location=device)

# 步骤2：从 checkpoint 读取配置（或从 state_dict 推断）
config = checkpoint.get("config") or {}
vocab_size = config.get("vocab_size", 50257)
max_seq_len = config.get("max_seq_len", 128)

# 步骤3：构建模型（必须与训练时结构一致）
model = SimpleGPT(
    vocab_size=vocab_size,
    d_model=config.get("d_model", 256),
    num_heads=config.get("num_heads", 4),
    num_layers=config.get("num_layers", 2),
    max_seq_len=max_seq_len,
)

# 步骤4：加载权重
model.load_state_dict(checkpoint["model_state_dict"])
model.to(device)
model.eval()

# 步骤5：加载 tokenizer（真实数据训练的模型需要）
if vocab_size == 50257:
    from transformers import GPT2Tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
```

### 3.2 文本生成（generate_text）

```python
# 步骤1：编码输入
prompt_ids = tokenizer.encode("Hello, how are you?")
prompt_ids = torch.tensor([prompt_ids], device=device)

# 步骤2：自回归生成（逐个预测下一个 token）
generated = prompt_ids.clone()
for _ in range(max_length):
    logits = model(generated)  # [1, seq_len, vocab_size]
    next_token_logits = logits[0, -1, :] / temperature  # 最后一个位置的 logits
    
    # 重复惩罚：降低已出现 token 的概率
    for token_id in set(generated[0].tolist()):
        next_token_logits[token_id] /= repetition_penalty
    
    # 采样下一个 token
    probs = torch.softmax(next_token_logits, dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)
    generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)
    
    # 遇到结束 token 停止
    if next_token.item() == tokenizer.eos_token_id:
        break

# 步骤3：解码输出
result = tokenizer.decode(generated[0].cpu().tolist())
```

### 3.3 使用方式

**单次推理**：
```bash
python scripts/inference.py \
    --prompt "Hello, how are you?" \
    --temperature 0.8 \
    --repetition_penalty 1.3
```

**对话模式**：
```bash
python scripts/chat.py
# 然后输入问题，模型会生成回答
```

---

## 四、数据流与形状变化速查

### 训练时

```
原始文本: "Hello world"
    ↓ tokenizer.encode()
token ids: [15496, 995]
    ↓ DataLoader (batch_size=4)
batch: [4, seq_len]  # 例如 [4, 128]
    ↓ 生成 labels
input_ids: [4, seq_len-1]  # [4, 127]
labels:    [4, seq_len-1]  # [4, 127]
    ↓ model(input_ids)
logits: [4, seq_len-1, vocab_size]  # [4, 127, 50257]
    ↓ criterion(logits.view(-1, vocab_size), labels.view(-1))
loss: 标量
```

### 推理时

```
用户输入: "Hello"
    ↓ tokenizer.encode()
prompt_ids: [15496]
    ↓ model(prompt_ids)
logits: [1, 1, vocab_size]
    ↓ 采样最后一个位置
next_token: [995]  # "world"
    ↓ 拼接
generated: [15496, 995]
    ↓ 继续生成...
generated: [15496, 995, 1234, ...]
    ↓ tokenizer.decode()
输出文本: "Hello world ..."
```

---

## 五、关键文件与函数对照表

| 功能 | 文件 | 关键函数/类 |
|------|------|------------|
| **数据加载** | `scripts/train_with_real_data.py` | `load_wikitext2()` |
| **模型定义** | `src/models/transformer.py` | `SimpleGPT`, `TransformerEncoder` |
| **训练循环** | `src/training/trainer.py` | `Trainer.train_epoch()`, `Trainer.validate()` |
| **模型加载** | `scripts/inference.py` | `load_model()` |
| **文本生成** | `scripts/inference.py` | `generate_text()` |
| **对话交互** | `scripts/chat.py` | `main()`（调用 `load_model` + `generate_text`） |

---

## 六、完整命令示例

### 从头到尾跑一遍

```bash
# 1. 训练（真实数据）
python scripts/train_with_real_data.py \
    --dataset wikitext2 \
    --d_model 256 \
    --num_layers 2 \
    --num_heads 4 \
    --batch_size 4 \
    --num_epochs 5 \
    --use_amp

# 2. 单次推理
python scripts/inference.py --prompt "Hello, how are you?"

# 3. 对话模式
python scripts/chat.py
```

---

## 七、常见问题速查

| 问题 | 原因 | 解决 |
|------|------|------|
| **加载模型报 shape 不匹配** | checkpoint 的 vocab_size / max_seq_len 与推理脚本不一致 | 已修复：自动从 checkpoint 读取 config |
| **生成一直重复 "are are are"** | 小模型 + 少训练轮数 | 使用 `--repetition_penalty 1.3`，或加大模型/多训几轮 |
| **显存不足** | batch_size 太大 | 减小 `--batch_size` 或启用 `--use_amp` |
| **数据加载慢** | 首次下载数据集 | 正常，HuggingFace 会自动下载并缓存 |

---

这份流程梳理覆盖了从数据到训练到使用的完整链路。你可以对照代码逐段理解每一步在做什么。
