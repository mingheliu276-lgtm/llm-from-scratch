# 数据集和模型能力说明

## 📊 当前数据集情况

### 1. 虚拟数据集（当前使用）

**位置**: `scripts/train.py` 中的 `create_dummy_dataset()`

**内容**: 
- 随机生成的token序列（1-10000之间的随机整数）
- 1000个样本，每个128个tokens
- **用途**: 仅用于测试训练流程，不包含真实语义

**局限性**: 
- ❌ 没有真实文本内容
- ❌ 无法学习语言知识
- ❌ 训练出的模型无法回答实际问题

### 2. 真实数据集（推荐使用）

为了训练一个能回答问题的模型，你需要使用真实文本数据集：

#### 推荐数据集：

1. **WikiText-2** (小规模，适合学习)
   - 维基百科文章
   - 约4MB文本
   - 适合RTX 5060训练

2. **OpenWebText** (大规模)
   - Reddit链接的网页内容
   - 约38GB文本
   - 需要云端训练

3. **中文数据集**
   - **WuDaoCorpus** (中文)
   - **CLUECorpus** (中文)
   - **中文维基百科**

## 🤖 模型能力

### 当前模型架构：SimpleGPT

**类型**: 语言模型（Language Model）
**任务**: 预测下一个token（Next Token Prediction）

### 训练后能做什么？

1. **文本生成** ✅
   - 给定一段文本，继续生成后续内容
   - 例如：输入"今天天气"，生成"今天天气很好"

2. **问答** ⚠️ (需要特殊训练)
   - 基础版本：可以基于训练数据中的模式回答问题
   - 更好的方式：使用RAG（检索增强生成）或微调

3. **文本补全** ✅
   - 自动补全句子
   - 代码补全（如果训练数据包含代码）

### 局限性

- ❌ **当前虚拟数据集训练的模型**：无法回答真实问题（因为数据是随机的）
- ⚠️ **小模型（<10M参数）**：能力有限，只能处理简单任务
- ⚠️ **需要真实数据集**：才能学习语言知识

## 🚀 下一步：使用真实数据集

查看 `scripts/train_with_real_data.py` 了解如何使用真实数据集训练。
