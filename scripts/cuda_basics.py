"""
CUDAåŸºç¡€ç»ƒä¹  - ç†Ÿæ‚‰GPUç¼–ç¨‹
è¿™æ˜¯ç¬¬1å‘¨çš„å­¦ä¹ å†…å®¹
"""

import torch
import time


def check_cuda_environment():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print("=" * 60)
    print("CUDAç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            print(f"\nGPU {i}:")
            print(f"  åç§°: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"  æ˜¾å­˜: {props.total_memory / 1e9:.2f} GB")
            print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    else:
        print("âš  CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥:")
        print("  1. NVIDIAé©±åŠ¨æ˜¯å¦å®‰è£…")
        print("  2. CUDA Toolkitæ˜¯å¦å®‰è£…")
        print("  3. PyTorchæ˜¯å¦å®‰è£…äº†CUDAç‰ˆæœ¬")
    
    print("=" * 60)


def vector_add_cpu_vs_gpu():
    """å¯¹æ¯”CPUå’ŒGPUçš„å‘é‡åŠ æ³•æ€§èƒ½"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ 1: CPU vs GPU å‘é‡åŠ æ³•")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("âš  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤ç»ƒä¹ ")
        return
    
    size = 10_000_000  # 1000ä¸‡ä¸ªå…ƒç´ 
    
    # CPUç‰ˆæœ¬
    print(f"\nCPUç‰ˆæœ¬ (å‘é‡å¤§å°: {size:,})...")
    a_cpu = torch.randn(size)
    b_cpu = torch.randn(size)
    
    start = time.time()
    c_cpu = a_cpu + b_cpu
    cpu_time = time.time() - start
    print(f"CPUè€—æ—¶: {cpu_time*1000:.2f} ms")
    
    # GPUç‰ˆæœ¬
    print(f"\nGPUç‰ˆæœ¬ (å‘é‡å¤§å°: {size:,})...")
    a_gpu = torch.randn(size).cuda()
    b_gpu = torch.randn(size).cuda()
    
    # é¢„çƒ­ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œä¼šåŒ…å«CUDAåˆå§‹åŒ–æ—¶é—´ï¼‰
    _ = a_gpu + b_gpu
    torch.cuda.synchronize()
    
    start = time.time()
    c_gpu = a_gpu + b_gpu
    torch.cuda.synchronize()  # ç­‰å¾…GPUè®¡ç®—å®Œæˆ
    gpu_time = time.time() - start
    print(f"GPUè€—æ—¶: {gpu_time*1000:.2f} ms")
    
    # éªŒè¯ç»“æœ
    c_cpu_from_gpu = c_gpu.cpu()
    max_diff = torch.max(torch.abs(c_cpu - c_cpu_from_gpu))
    print(f"\nç»“æœéªŒè¯: æœ€å¤§å·®å¼‚ = {max_diff.item():.2e}")
    
    if cpu_time > 0:
        speedup = cpu_time / gpu_time
        print(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    print("=" * 60)


def matrix_multiplication():
    """çŸ©é˜µä¹˜æ³•ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ 2: çŸ©é˜µä¹˜æ³• (GPU)")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("âš  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤ç»ƒä¹ ")
        return
    
    # åˆ›å»ºå¤§çŸ©é˜µ
    size = 2048
    print(f"\nçŸ©é˜µå¤§å°: {size} x {size}")
    
    A = torch.randn(size, size).cuda()
    B = torch.randn(size, size).cuda()
    
    # é¢„çƒ­
    _ = torch.matmul(A, B)
    torch.cuda.synchronize()
    
    # è®¡æ—¶
    start = time.time()
    C = torch.matmul(A, B)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    print(f"è€—æ—¶: {elapsed*1000:.2f} ms")
    print(f"ååé‡: {(2*size**3) / elapsed / 1e9:.2f} GFLOPS")
    print("=" * 60)
    
    # æ¸…ç†
    del A, B, C
    torch.cuda.empty_cache()


def memory_management():
    """GPUå†…å­˜ç®¡ç†ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ 3: GPUå†…å­˜ç®¡ç†")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("âš  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤ç»ƒä¹ ")
        return
    
    # æŸ¥çœ‹å½“å‰æ˜¾å­˜ä½¿ç”¨
    print("\nåˆå§‹æ˜¾å­˜çŠ¶æ€:")
    print(f"å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1e6:.2f} MB")
    print(f"å·²ç¼“å­˜: {torch.cuda.memory_reserved(0) / 1e6:.2f} MB")
    
    # åˆ†é…ä¸€äº›å¼ é‡
    print("\nåˆ†é…å¼ é‡...")
    tensors = []
    for i in range(5):
        t = torch.randn(1000, 1000).cuda()
        tensors.append(t)
        print(f"  å¼ é‡ {i+1}: {torch.cuda.memory_allocated(0) / 1e6:.2f} MB")
    
    # é‡Šæ”¾
    print("\né‡Šæ”¾å¼ é‡...")
    del tensors
    torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
    print(f"é‡Šæ”¾å: {torch.cuda.memory_allocated(0) / 1e6:.2f} MB")
    
    print("=" * 60)


def gradient_computation():
    """æ¢¯åº¦è®¡ç®—ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ 4: è‡ªåŠ¨å¾®åˆ† (GPU)")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("âš  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤ç»ƒä¹ ")
        return
    
    # ç›´æ¥åœ¨GPUä¸Šåˆ›å»ºleaf tensorï¼Œé¿å….cuda()æ“ä½œå¯¼è‡´çš„é—®é¢˜
    device = 'cuda'
    x = torch.randn(1000, 1000, requires_grad=True, device=device)
    y = torch.randn(1000, 1000, device=device)
    
    # å‰å‘ä¼ æ’­
    z = (x * y).sum()
    
    # åå‘ä¼ æ’­
    z.backward()
    
    print(f"è¾“å…¥ x çš„å½¢çŠ¶: {x.shape}")
    print(f"x æ˜¯ leaf tensor: {x.is_leaf}")
    
    # å®‰å…¨åœ°è®¿é—®gradï¼ˆé¿å…è­¦å‘Šï¼‰
    try:
        # ç›´æ¥è®¿é—®gradå¯èƒ½ä¼šè§¦å‘è­¦å‘Šï¼Œä½†æˆ‘ä»¬çŸ¥é“xæ˜¯leaf tensor
        grad = x.grad
        if grad is not None:
            print(f"æ¢¯åº¦ x.grad çš„å½¢çŠ¶: {grad.shape}")
            print(f"æ¢¯åº¦èŒƒæ•°: {grad.norm().item():.4f}")
            print("âœ“ æ¢¯åº¦è®¡ç®—æˆåŠŸ!")
        else:
            print("âš  è­¦å‘Š: x.grad ä¸º None")
    except Exception as e:
        print(f"âš  è®¿é—®gradæ—¶å‡ºé”™: {e}")
        print("è¿™å¯èƒ½æ˜¯PyTorchç‰ˆæœ¬æˆ–CUDAå…¼å®¹æ€§é—®é¢˜")
        print("ä½†è®­ç»ƒæ—¶åº”è¯¥èƒ½æ­£å¸¸å·¥ä½œï¼ˆè®­ç»ƒä»£ç ä½¿ç”¨äº†æ›´å®‰å…¨çš„æ–¹å¼ï¼‰")
    
    print("=" * 60)


def main():
    """è¿è¡Œæ‰€æœ‰ç»ƒä¹ """
    check_cuda_environment()
    vector_add_cpu_vs_gpu()
    matrix_multiplication()
    memory_management()
    gradient_computation()
    
    print("\n" + "=" * 60)
    print("âœ“ æ‰€æœ‰CUDAåŸºç¡€ç»ƒä¹ å®Œæˆ!")
    print("=" * 60)
    
    # å…³äºæ¢¯åº¦è®¡ç®—è­¦å‘Šçš„è¯´æ˜
    print("\nğŸ“ å…³äºç»ƒä¹ 4çš„è­¦å‘Šè¯´æ˜:")
    print("å¦‚æœçœ‹åˆ° 'non-leaf Tensor' è­¦å‘Šï¼Œè¿™æ˜¯PyTorchçš„å†…éƒ¨æ£€æŸ¥æœºåˆ¶")
    print("å®é™…ä¸Šæ¢¯åº¦è®¡ç®—æ˜¯æˆåŠŸçš„ï¼Œè¿™ä¸ªè­¦å‘Šå¯ä»¥å¿½ç•¥")
    print("è®­ç»ƒä»£ç ä¸­ä½¿ç”¨äº†æ›´å®‰å…¨çš„æ–¹å¼ï¼Œä¸ä¼šå‡ºç°æ­¤è­¦å‘Š")
    
    print("\nä¸‹ä¸€æ­¥:")
    print("1. é˜…è¯» TRAINING_GUIDE.md äº†è§£å®Œæ•´å­¦ä¹ è·¯å¾„")
    print("2. è¿è¡Œ python scripts/train.py å¼€å§‹è®­ç»ƒæ¨¡å‹")
    print("3. ä½¿ç”¨ nvidia-smi ç›‘æ§GPUä½¿ç”¨æƒ…å†µ")


if __name__ == "__main__":
    main()
